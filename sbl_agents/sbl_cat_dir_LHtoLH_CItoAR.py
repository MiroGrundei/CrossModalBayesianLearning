import argparse
from utils.helpers import *


class SBL_Cat_Dir():
    """
    DESCRIPTION: Categorical Dirichlet Bayesian Sequential Learning Agent
        * Agent parses a sequence previously generated by HHMM
        * It updates conjugate Cat-Dir posterior with new evidence
        * It calculates different surprise measures as the observations come in
        * This variant specifically handles either
        A) a binary sequence (e.g., Low and High stimuli) and updates 2x2 transition matrix: model_type = "LHtoLH"
            Transitions: Low->Low,Low->High,High->Low,High->High
        B) a sequence with 4 possible outcomes (e.g., Low|Congruent, Low|Incongruent, High|Congruent, High|Incongruent)
            and updates a 2x2 transition matrix: model_type = "CItoAR"
            Transitions: Congruent->Alternation, Congruent->Repetition, Incongruent->Alternation,Incongruent->Repetition
    INPUT: Sequence and Exponentially Weighted forgetting parameter
    OUTPUT: Predictive surprisal, Bayesian surprisal, Confidence-corrected surprisal
    [t, o_t, s_t, Predictive_Surprise, Bayesian_Surprise, Confidence_Corrected_Surprise]
    """
    def __init__(self, seq, tau, model_type="LHtoLH", verbose=False):
        # Type of transition model: "LHtoLH" or "CItoAR"
        self.type = model_type

        # Initialize SBL-learned sequence and exponential forgetting parameter
        self.sequence = seq.astype(int)
        self.T = len(seq)

        self.type = model_type
        self.tau = tau
        self.verbose = verbose

        self.no_obs = np.unique(seq).shape[0]
        self.stim_ind = np.zeros((self.T, self.no_obs))

        # Construct matrix where col represents binary ind of specific stim at t
        for t in range(self.T):
            self.stim_ind[t, self.sequence[t]] = 1

        # TP: Generate T-dim vectors indicating transition from state i
        self.transitions = np.zeros((self.T, self.no_obs))
        for t in range(1, self.T):
            for n in range(0, self.no_obs):
                self.transitions[t, n] = (self.sequence[t-1] == n)

        # Generate one T matrix with all discounting values
        self.exp_forgetting = np.exp(-self.tau*np.arange(self.T)[::-1])

        # Generate matrix of concentration parameters alpha
        self.alphas = np.ones((self.no_obs, self.no_obs)) / self.no_obs

    def update_posterior(self):
        exp_weighting = self.exp_forgetting[-(self.t+1):]

        if self.t == 0:
            # print("Can't update posterior with only one observation - need two!")
            # reduce priors for 2 by 2 transition matrix (devide by 4)
            self.alphas = np.ones((self.no_obs, self.no_obs)) / self.no_obs
        else:
            for i in range(self.no_obs):
                for j in range(self.no_obs):
                    # from-to alphas
                    self.alphas[i, j] = 1/self.no_obs + np.dot(exp_weighting, self.stim_ind[:self.t+1, j]*self.transitions[:self.t+1, i])

    def posterior_predictive(self, alphas):
        return np.array([alpha/alphas.sum(axis=1) for alpha in alphas.T]).T

    def naive_posterior(self, ind):
        # Performs single step update on naive/flat prior by adding 1 to alpha corresponding to observation
        naive_update = self.alpha_naive_init.copy()
        naive_update[ind] += 1
        return naive_update

    def predictive_surprisal(self, alphas, ind):
        return -np.log(self.posterior_predictive(alphas)[ind])

    def bayesian_surprisal(self, alphas_old, alphas):
        return kl_dir(alphas_old, alphas)

    def corrected_surprisal(self, alphas_old, ind):
        return kl_dir(alphas_old, self.naive_posterior(ind))

    def compute_surprisal(self, max_T, verbose_surprisal=False):

        if verbose_surprisal:
            print("{}: Computing different surprisal measures for {} timesteps.".format(self.type, max_T))

        if self.type == 'LHtoLH':
            self.alpha_naive_init = self.alphas.copy()
        elif self.type == 'CItoAR':
            # reduce alphas to 2 by 2 by summing over entries of transition matrix (alphas naive)
            self.alpha_naive_init = np.zeros((2, 2))
            tples = [(0, 0), (0, 1), (2, 2), (2, 3)]
            self.alpha_naive_init[0, 0] = np.array([self.alphas[tpl] for tpl in tples]).sum()
            tples = [(0, 2), (0, 3), (2, 0), (2, 1)]
            self.alpha_naive_init[0, 1] = np.array([self.alphas[tpl] for tpl in tples]).sum()
            tples = [(1, 0), (1, 1), (3, 2), (3, 3)]
            self.alpha_naive_init[1, 0] = np.array([self.alphas[tpl] for tpl in tples]).sum()
            tples = [(1, 2), (1, 3), (3, 0), (3, 1)]
            self.alpha_naive_init[1, 1] = np.array([self.alphas[tpl] for tpl in tples]).sum()

        # initiate variables to be saved
        results = []
        if self.type == 'LHtoLH':
            distr_params = np.zeros((max_T, self.no_obs, self.no_obs))
        if self.type == 'CItoAR':
            distr_params = np.zeros((max_T, round(self.no_obs/2), round(self.no_obs/2)))

        for t in range(max_T):
            # loop over the full sequence and compute surprisal iteratively

            if self.type == 'LHtoLH':
                alphas_old = self.alphas.copy()
            elif self.type == 'CItoAR':
                # reduce alphas to 2 by 2 by summing over entries of transition matrix (alphas pre-update)
                red_alphas_old = np.zeros((2, 2))
                tples = [(0, 0), (0, 1), (2, 2), (2, 3)]
                red_alphas_old[0, 0] = np.array([self.alphas[tpl] for tpl in tples]).sum()
                tples = [(0, 2), (0, 3), (2, 0), (2, 1)]
                red_alphas_old[0, 1] = np.array([self.alphas[tpl] for tpl in tples]).sum()
                tples = [(1, 0), (1, 1), (3, 2), (3, 3)]
                red_alphas_old[1, 0] = np.array([self.alphas[tpl] for tpl in tples]).sum()
                tples = [(1, 2), (1, 3), (3, 0), (3, 1)]
                red_alphas_old[1, 1] = np.array([self.alphas[tpl] for tpl in tples]).sum()

            # update posterior at current time step
            self.t = t
            self.update_posterior()

            # from and to stimulus transition index
            ind = (np.argmax(self.transitions[self.t, :]), np.argmax(self.stim_ind[self.t, :]))

            if self.type == 'LHtoLH':
                # compute surprisal
                PS_temp = self.predictive_surprisal(alphas_old, ind)
                BS_temp = self.bayesian_surprisal(alphas_old, self.alphas)
                CS_temp = self.corrected_surprisal(alphas_old, ind)
            elif self.type == 'CItoAR':
                # reduce alphas to 2 by 2 by summing over entries of transition matrix (alphas post-update)
                red_alphas = np.zeros((2, 2))
                tples = [(0, 0), (0, 1), (2, 2), (2, 3)]
                red_alphas[0, 0] = np.array([self.alphas[tpl] for tpl in tples]).sum()
                tples = [(0, 2), (0, 3), (2, 0), (2, 1)]
                red_alphas[0, 1] = np.array([self.alphas[tpl] for tpl in tples]).sum()
                tples = [(1, 0), (1, 1), (3, 2), (3, 3)]
                red_alphas[1, 0] = np.array([self.alphas[tpl] for tpl in tples]).sum()
                tples = [(1, 2), (1, 3), (3, 0), (3, 1)]
                red_alphas[1, 1] = np.array([self.alphas[tpl] for tpl in tples]).sum()

                self.red_alphas = red_alphas

                # change index for reduced transition matrix (2 by 2)
                if ind[0] == 0:
                    if ind[1] == 0 or ind[1] == 1:
                        red_ind = (0, 0)
                    elif ind[1] == 2 or ind[1] == 3:
                        red_ind = (0, 1)
                elif ind[0] == 1:
                    if ind[1] == 0 or ind[1] == 1:
                        red_ind = (1, 0)
                    elif ind[1] == 2 or ind[1] == 3:
                        red_ind = (1, 1)
                elif ind[0] == 2:
                    if ind[1] == 0 or ind[1] == 1:
                        red_ind = (0, 1)
                    elif ind[1] == 2 or ind[1] == 3:
                        red_ind = (0, 0)
                elif ind[0] == 3:
                    if ind[1] == 0 or ind[1] == 1:
                        red_ind = (1, 1)
                    elif ind[1] == 2 or ind[1] == 3:
                        red_ind = (1, 0)

                # compute surprisal
                PS_temp = self.predictive_surprisal(red_alphas_old, red_ind)
                BS_temp = self.bayesian_surprisal(red_alphas_old, self.red_alphas)
                CS_temp = self.corrected_surprisal(red_alphas_old, red_ind)

            if verbose_surprisal:
                print("{} - t={}: PS={}, BS={}, CS={}".format(self.type, t+1, round(PS_temp, 4),  round(BS_temp, 4), round(CS_temp, 4)))

            # save results
            temp = [t, self.sequence[t], PS_temp, BS_temp, CS_temp]
            if self.type == 'LHtoLH':
                distr_params[t, :, :] = self.alphas
            elif self.type == 'CItoAR':
                distr_params[t, :, :] = self.red_alphas
            results.append(temp)

        if verbose_surprisal:
            print("{}: Done computing surprisal measures for all {} timesteps.".format(self.type, self.T))
        return np.asarray(results), np.asarray(distr_params)


def main(seq, tau, model_type, save_results=False, title="temp", verbose=False):
    # Compute Surprisal for all time steps for Transition Prob CatDir Model
    CD_SBL_temp = SBL_Cat_Dir(seq, tau, model_type, verbose)
    results = CD_SBL_temp.compute_surprisal(max_T=CD_SBL_temp.T, verbose_surprisal=verbose)

    time = results[:, 0]
    sequence = results[:, 1]
    PS = results[:, 3]
    BS = results[:, 4]
    CS = results[:, 5]

    if save_results:
        results_formatted = {"time": time,
                             "sequence": sequence,
                             "predictive_surprise": PS,
                             "bayesian_surprise": BS,
                             "confidence_corrected_surprise": CS}

        save_obj(results_formatted, results_dir + title)
        print("Saved in File: {}".format(results_dir + title))
    else:
        return PS, BS, CS


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-file', '--sample_file', action="store",
                        default="temporary_sample_title", type=str,
                        help='Title of file in which sequence in stored')
    parser.add_argument('-tau', '--forget_param', action="store",
                        default=0., type=float,
                        help='Exponentially weighting parameter for memory/posterior updating')
    parser.add_argument('-model', '--model', action="store", default="SP",
                        type=str,
                        help='Categorical Dirichlet Probability Model (SP, AP, TP)')
    parser.add_argument('-pkl_in', '--pickle', action="store_true", help='Load matlab sequence file.')
    parser.add_argument('-T', '--test', action="store_true", help='Run tests.')
    parser.add_argument('-S', '--save', action="store_true", help='Save results to array.')
    parser.add_argument('-v', '--verbose',
                        action="store_true",
                        default=False,
                        help='Get status printed out')

    args = parser.parse_args()

    if args.pickle:
        sample, meta = load_obj(results_dir + args.sample_file + ".pkl")
    else:
        sample, meta = load_obj(results_dir + args.sample_file + ".mat")

    seq = sample[:, 2]

    tau = args.forget_param
    model = args.model

    run_test = args.test
    save_results = args.save
    verbose = args.verbose

    main(seq, tau, model, save_results=save_results, title="DC_TP_" + model + "_" + args.sample_file, verbose=False)

    """
    How to run:
        pythonw seq_gen.py -t S1_800 -obs_change 0.75 0.15 0.85 0.25 0.5 0.75 0.25 0.5 -order 2 -matlab -seq 500
        pythonw sbl_cat_dir.py -file S1_800 -S -model LHtoLH
    """
